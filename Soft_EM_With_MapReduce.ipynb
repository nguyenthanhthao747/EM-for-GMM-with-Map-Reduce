{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft EM for GMM with Map-Reduce\n",
    "Implementation a Map-Reduce version of soft EM for GMM in Spark (using Python 3.6.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data_file.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# to compute covariance matrix\n",
    "from scipy.stats import multivariate_normal as mvn \n",
    "\n",
    "#libray pyspark to work with spark\n",
    "from pyspark import SparkContext \n",
    "\n",
    "from __future__ import print_function \n",
    "from __future__ import division # interger division vs float division\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1,x2,x3,x4\r\n",
      "2.9,6.2,1.3,4.3\r\n",
      "3.4,6.2,2.3,5.4\r\n",
      "3.0,5.6,1.3,4.1\r\n",
      "2.2,6.2,1.5,4.5\r\n",
      "3.0,6.5,2.2,5.8\r\n",
      "3.0,6.6,1.4,4.4\r\n",
      "3.4,4.8,0.2,1.6\r\n",
      "3.7,5.1,0.4,1.5\r\n",
      "2.4,5.5,1.1,3.8\r\n"
     ]
    }
   ],
   "source": [
    "# stop all spark context then creat new\n",
    "sc.stop() \n",
    "sc = SparkContext(appName = \"EMMapReduce\") # create a new SparkContext object\n",
    "#read data\n",
    "lines = sc.textFile(\"./data_file.csv\") \n",
    "\n",
    "!head ./data_file.csv # view first few line of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the header\n",
    "header = lines.first()\n",
    "data = lines.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converet each line of the text data file into a NumPy Array of float numbers\n",
    "data = data.map(lambda line: np.array([float(l) for l in line.split(',')])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 3.4,  6.2,  2.3,  5.4]),\n",
       " array([ 3. ,  5.6,  1.3,  4.1]),\n",
       " array([ 2.2,  6.2,  1.5,  4.5]),\n",
       " array([ 3. ,  6.5,  2.2,  5.8])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of data points\n",
    "N = data.count() # \n",
    "# number of dimenstions\n",
    "D = data.collect()[0].shape[0]\n",
    "\n",
    "data.collect()[1:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement soft Expectation Maximisation for Gaussian Mixture Model with Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clusterFunc  for each x to calculate all sigma, mu and posterior for that x and K\n",
    "def clusterFunc(x, K_input, muHat, sigmaHat, NkHat):\n",
    "    K = K_input\n",
    "    post_x = list(range(K))\n",
    "    sigma_hat_x = sigmaHat.copy()\n",
    "    mu_hat_x = muHat.copy()\n",
    "    \n",
    "    tempt_muHat = muHat\n",
    "    \n",
    "    #find the posterior of x in cluster k\n",
    "    for k in range(K):\n",
    "        post_x[k] = mvn.pdf(\n",
    "            x, \n",
    "            mean = np.squeeze(np.array(muHat[k])),  \n",
    "            cov=np.reshape(sigmaHat[k], (D,D))) * NkHat[k]\n",
    "    \n",
    "    #normalization\n",
    "    post_x = post_x / np.sum(post_x)\n",
    "        \n",
    "    #for each cluster k, calculate the sigma and mu for that x\n",
    "    for k in range(K):\n",
    "        sigma_hat_x[k] = np.dot(np.matrix(x).T, post_x[k]*np.matrix(x)).reshape(1,16)\n",
    "        mu_hat_x[k] = (post_x[k]*np.matrix(x))\n",
    "    \n",
    "    return np.matrix(post_x), mu_hat_x, sigma_hat_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map reduce function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emMapReduceFunc(K_input):\n",
    "    ############### set initial##############\n",
    "    #set number of cluster\n",
    "    K = K_input \n",
    "    # set maximum number of iterations\n",
    "    max_i = 100 \n",
    "    \n",
    "    # condition to stop the loop\n",
    "    prgrs_min = float(0.001) \n",
    "    prgrs = [float('+inf')]\n",
    "\n",
    "    phiHat = [1/K]*3  # assume all clusters have the same size (we will update this later on)\n",
    "    \n",
    "    #initial the number of data points in each cluster\n",
    "    NkHat = [N/K] * K \n",
    "\n",
    "    # randomly select cluster centers from the data set\n",
    "    muHat = np.matrix(data.takeSample(withReplacement=False, num=K, seed=150))\n",
    "\n",
    "    # create empty covariance matrices\n",
    "    sigmaHat = np.matrix(np.zeros((K, D*D))) \n",
    "    # initialize the covariance matrix with identity matrix\n",
    "    for k in range(K):\n",
    "        sigmaHat[k] = np.identity(D).flatten()\n",
    "        \n",
    "    ######################start for EM map reduce#########################\n",
    "    for i in range(max_i):\n",
    "        #store the previous cluster centres\n",
    "        previous_mu_h = muHat.copy() \n",
    "        prgrs.append(float('+inf'))\n",
    "\n",
    "        #map each element x to calculate the sigma, mu and Nk \n",
    "        indx_sum_one = data.map(lambda x: (clusterFunc(x, K, muHat, sigmaHat, NkHat)[0], clusterFunc(x, K, muHat, sigmaHat, NkHat)[1], clusterFunc(x, K, muHat, sigmaHat, NkHat)[2] ))\n",
    "        #reduce, sum all the sigma, mu and nk for all K \n",
    "        reduced_results = indx_sum_one.reduce(lambda x1, x2: (x1[0] + x2[0], x1[1] + x2[1],x1[2] + x2[2]))\n",
    "\n",
    "        #now from the reduced_results, calculate and update NkHat, muHat and sigmaHat\n",
    "        for k in range(K):\n",
    "        # print(reduced_results[0][0, k])   \n",
    "            NkHat[k] = reduced_results[0][0, k]\n",
    "            muHat[k] = reduced_results[1][k] / NkHat[k]\n",
    "            sigmaHat[k] = (reduced_results[2][k] / NkHat[k]) - (np.dot(np.matrix(muHat[k]).T, np.matrix(muHat[k]))).flatten()\n",
    "            phiHat[k] = NkHat[k]/N\n",
    "            \n",
    "     # check termination threshold\n",
    "        prgrs[i] = 0\n",
    "        # calculate sum of distances between the cureent location of the clusters and their previous locations\n",
    "        for k in range(K):\n",
    "            prgrs[i] += np.sum(previous_mu_h[k] - muHat[k])**2\n",
    "    #   print(prgrs[i])\n",
    "\n",
    "        if prgrs[i] <= prgrs_min: break\n",
    "            \n",
    "    return muHat, NkHat, phiHat, sigmaHat         \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will set the number of clusters to 3 and run with the data file above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = emMapReduceFunc(3)\n",
    "muHat = model[0]\n",
    "NkHat = model[1]\n",
    "phiHat = model[2]\n",
    "sigmaHat = model[3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centres:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.43</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.01</td>\n",
       "      <td>6.40</td>\n",
       "      <td>2.09</td>\n",
       "      <td>5.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.80</td>\n",
       "      <td>6.18</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3\n",
       "0  3.43  5.01  0.25  1.46\n",
       "1  3.01  6.40  2.09  5.36\n",
       "2  2.80  6.18  1.44  4.65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the cluster position\n",
    "clusterCenters = pd.DataFrame(muHat)\n",
    "clusterCenters = clusterCenters.round(2)\n",
    "print(\"Cluster centres:\")\n",
    "clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in each cluster:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  49.99\n",
       "1  36.52\n",
       "2  63.49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the size for each cluster\n",
    "clusterSizes = pd.DataFrame(NkHat)\n",
    "clusterSizes = clusterSizes.round(2)\n",
    "print(\"Number of data points in each cluster:\")\n",
    "clusterSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster probability:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  0.33\n",
       "1  0.24\n",
       "2  0.42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster Probability\n",
    "clusterProbability = pd.DataFrame(phiHat)\n",
    "clusterProbability = clusterProbability.round(2)\n",
    "print(\"cluster probability:\")\n",
    "clusterProbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([clusterCenters, clusterSizes, clusterProbability], axis=1)\n",
    "result.columns = ['X1mu','X2mu','X2mu','X2mu','Size', \"ClusterProbability\"]\n",
    "result.index.name = \"K\"\n",
    "result = result.reset_index()\n",
    "result[\"K\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters of the model:\n",
      "\n",
      "K  X1mu  X2mu  X2mu  X2mu   Size  ClusterProbability\n",
      "2  3.01  6.40  2.09  5.36  36.52                0.24\n",
      "1  3.43  5.01  0.25  1.46  49.99                0.33\n",
      "3  2.80  6.18  1.44  4.65  63.49                0.42\n"
     ]
    }
   ],
   "source": [
    "print(\"Learnt parameters of the model:\")\n",
    "sort_results = result.sort_values(by = ['Size'], ascending = True)\n",
    "print()\n",
    "print(sort_results.to_string(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
